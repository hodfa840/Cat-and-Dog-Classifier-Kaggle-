{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b7ed1e651ae29cfa0814a55f59e284dc",
     "grade": false,
     "grade_id": "cell-5690119ead85e67e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Checklist for submission\n",
    "\n",
    "It is extremely important to make sure that:\n",
    "\n",
    "1. Everything runs as expected (no bugs when running cells);\n",
    "2. The output from each cell corresponds to its code (don't change any cell's contents without rerunning it afterwards);\n",
    "3. All outputs are present (don't delete any of the outputs);\n",
    "4. Fill in all the places that say `# YOUR CODE HERE`, or \"**Your answer:** (fill in here)\".\n",
    "5. Never copy/paste any notebook cells. Inserting new cells is allowed, but it should not be necessary.\n",
    "6. The notebook contains some hidden metadata which is important during our grading process. **Make sure not to corrupt any of this metadata!** The metadata may for example be corrupted if you copy/paste any notebook cells, or if you perform an unsuccessful git merge / git pull. It may also be pruned completely if using Google Colab, so watch out for this. Searching for \"nbgrader\" when opening the notebook in a text editor should take you to the important metadata entries.\n",
    "7. If you need to have multiple parallel versions of this notebook, make sure not to move them to another directory.\n",
    "8. Although not forced to work exclusively in the course `conda` environment, you need to make sure that the notebook will run in that environment, i.e. that you have not added any additional dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a025ba528a4e9c11fc54be126fdffab0",
     "grade": false,
     "grade_id": "cell-5676bcf768a7f9be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Fill in group number and member names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME1 = \"Hoda Fakharzadehjahromy\"\n",
    "NAME2 = \"Emil Wiman\"\n",
    "GROUP = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "42f960a95815e1aa3ce8132fcec59cd9",
     "grade": false,
     "grade_id": "cell-a15fe781533d9590",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Check Python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "72b2403e87a33f87371b150984248355",
     "grade": false,
     "grade_id": "cell-2b9c2390ee464c39",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "You are not running Python 3.11. Make sure to run Python through the course Conda environment.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m python_version_tuple\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     python_version_tuple()[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m==\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m11\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are not running Python 3.11. Make sure to run Python through the course Conda environment.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: You are not running Python 3.11. Make sure to run Python through the course Conda environment."
     ]
    }
   ],
   "source": [
    "from platform import python_version_tuple\n",
    "\n",
    "assert (\n",
    "    python_version_tuple()[:2] == (\"3\", \"11\")\n",
    "), \"You are not running Python 3.11. Make sure to run Python through the course Conda environment.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f0dd7cbad727dec0308b03071cff6d79",
     "grade": false,
     "grade_id": "cell-8092c3fd452a3245",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# HA1 - Cats and dogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a54241ea89512f794ee9e366f2ef92f3",
     "grade": false,
     "grade_id": "cell-0235e816fc98b0f6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<img src=\"https://cdn.pixabay.com/photo/2015/05/20/10/03/cat-and-dog-775116_960_720.jpg\" alt=\"Image of cats and dogs\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "58c7c544e00ee606c78136237038020f",
     "grade": false,
     "grade_id": "cell-c4bb694612153106",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "For this home assignment, we'll use the Kaggle dataset for the [Dogs vs. Cats competition](https://www.kaggle.com/c/dogs-vs-cats). It is comprised of 25k colour images of dogs and cats. Our goal with this assignment will be to create a classifier that can discriminate between cats or dogs.\n",
    "\n",
    "The goal is to make sure that you all can independently create, train and evaluate a model using a popular deep learning framework. A secondary goal is also to expose you to GPU computing, either your own, by using a server/cluster, or via a cloud computing service. The focus is on implementing the models, and much of the surrounding code is provided for you. You are expected to understand the provided code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d0b2d411ee531671dcdef179e757f68",
     "grade": false,
     "grade_id": "cell-ee9e2aee031325a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Using your GPU\n",
    "\n",
    "### Strong recommendation:\n",
    "In order to make the most out of your GPU hours, first try solving the initial part of this notebook (tasks 0-3) on your own computer (these tasks can be solved on the CPU). Remaining tasks (4-6) will require GPU computing. While you are free to use any GPU resource, we recommend getting familiar with using a server, cluster, or cloud computing service.\n",
    "\n",
    "\n",
    "### Working efficiently:\n",
    "Training for several epochs just to have your code break at the last validation step is incredibly frustrating and inefficient. Good practice is to first test long training runs with a much simpler dry-run: a single epoch, a few batches etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3e1177cf9ec47bd5f9bb2ea87bdc741b",
     "grade": false,
     "grade_id": "cell-f7371c24b57c153e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Requirements:\n",
    "- Whenever we ask you to plot anything, be sure to add a title and label the axes. If you're plotting more than one curve in the same plot, also add a legend.\n",
    "- When we ask you to train an architecture, train it for a reasonable number of epochs. \"Reasonable\" here means you should be fairly confident that training for a higher number of epochs wouldn't impact your conclusions regarding the model's performance. When experimenting, a single epoch is often enough to tell whether your model setup has improved or not.\n",
    "\n",
    "\n",
    "**IMPORTANT NOTES:**\n",
    "- Some cells contain compiled tests. For them to work propery, make sure to keep variable names. For example, if you are asked to create a training dataset, and `train_dataset` is passed to the test, then you should complete the task by updating `train_dataset` variable in your code. But, of course, you can create auxiliary variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9f50e27a30d83bcebeb52a8ae43228e2",
     "grade": false,
     "grade_id": "cell-3ee6d24346a80d85",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 0. Imports\n",
    "\n",
    "In the following cell, add all the imports you'll use in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "41ab17075f0238045a6899741c255177",
     "grade": false,
     "grade_id": "cell-464a08ede00083a4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from utils.tests import ha1_tests\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "54010effebd0a025505ab189ffff6d6c",
     "grade": false,
     "grade_id": "cell-4821dc273028d702",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 1. Loading the data and preprocessing\n",
    "\n",
    "In this part we will set up the data used in this assignment. You need to download it, then we'll walk you through how to make a custom Pytorch dataset abstraction. The abstraction enables you to visualise and play around with the image data and to finally create data loaders, which are necessary for the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed23de42afdd3e244500329a899ead93",
     "grade": false,
     "grade_id": "cell-2ea049dea4713494",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The first step is to head to the [Kaggle website for the cats and dogs competition](https://www.kaggle.com/c/dogs-vs-cats/data) and download the data from there. You should download both the test and train folders together in one zip file (there is a `Download all` button at the bottom of the page). Unfortunately, you need to create a Kaggle account for this.\n",
    "\n",
    "**Only necessary for tasks 4-6**: Downloading the data to your local computer is quite straight-forward. If you have to upload the data to a cloud/server instance that might be a bit more tricky. There are a few ways to do it:\n",
    "\n",
    " - Jupyter Notebook upload function. When starting the notebook server with the command `jupyter notebook` you are directed to a main page. In the top right corner there is an upload button.\n",
    " - Using [`scp`](https://linuxize.com/post/how-to-use-scp-command-to-securely-transfer-files/) to copy files via an ssh connection.\n",
    " - Using the [Kaggle CLI](https://github.com/Kaggle/kaggle-api). We have added it to the conda environment.\n",
    "\n",
    "To begin with, download the data to your local computer and create a folder structure that resembles the following (obviously, the folder names are up to you):\n",
    "\n",
    "\n",
    "         small_train             small_val                train                   val\n",
    "              |                      |                      |                      |\n",
    "              |                      |                      |                      |\n",
    "        -------------          -------------          -------------          -------------\n",
    "        |           |          |           |          |           |          |           |\n",
    "        |           |          |           |          |           |          |           |\n",
    "      cats        dogs       cats        dogs       cats        dogs       cats        dogs\n",
    "\n",
    "\n",
    "The `small_train` and `small_val` folders have the training and validation samples for your smaller subset of the data, while the `train` and `val` folders contain all the samples you extracted from Kaggle's `train.zip`.\n",
    "This is just a convenient way of having a smaller dataset to play with for faster prototyping.\n",
    "\n",
    "We provide you a notebook that shows how to achieve this folder structure (`create_project_notebook_structure.ipynb`), starting from the original `dogs-vs-cats.zip` file that you download from Kaggle. If you do use that notebook, we encourage you to understand how each step is being done, so you can generalize this knowledge to new datasets you'll encounter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a517440fae4e8439bf39260ba20c3e36",
     "grade": false,
     "grade_id": "cell-89ba19509b952af2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(1 point)**\n",
    "\n",
    "For the smaller dataset, we advise you to use 70% of the data as training data (and thereby the remaining 30% for validation data). However, for the larger dataset, you should decide how to split between training and validation.\n",
    "\n",
    "- What percentage of the larger dataset did you decide to use for training/validation?\n",
    "- Motivate your decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c78fb7af1d8b50d9607cf49e87d1ce51",
     "grade": true,
     "grade_id": "cell-7f3b0dfbd90a14c1",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "834d9263037e9d6133fb6ca7faef761b",
     "grade": false,
     "grade_id": "cell-876ca7df88c9311f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Fill in the dataset paths (to be used later by your data loaders):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6cfb71bbe93c720e26315fa66b25d249",
     "grade": false,
     "grade_id": "cell-1b1314f2ab1b1d6b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Change the directories accordingly\n",
    "train_path = \"/your/path\"\n",
    "val_path = \"/your/path\"\n",
    "small_train_path = \"/your/path\"\n",
    "small_val_path = \"/your/path\"\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7fbc8bc2281dee12a9331450f3659e14",
     "grade": false,
     "grade_id": "cell-1d6ea64bca94a4ef",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 1.1 Dataset\n",
    "\n",
    "To create data loaders we first need to create a dataset abstraction class. The purpose of a data loader is to efficiently provide the CPU/GPU with mini-batches of data. We now work with data complex enough to actually warrant the use of data loaders. In particular, we don't want to load all images into memory at once.\n",
    "\n",
    "The data loader is an instance of the Pytorch [`DataLoader`](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader) which wraps a class that inherits from [`Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset), that we create ourselves.\n",
    "Image classification is such a common task that Pytorch provides a ready-to-use dataset class for this task, called [`ImageFolder`](https://pytorch.org/vision/stable/datasets.html?highlight=imagefolder#imagefolder).\n",
    "Using this class however, is rather opaque so for your understanding we will show you how to construct a custom dataset class. If you know this method, you will be able to create a data loader for any dataset you may encounter.\n",
    "\n",
    "We construct a class `DogsCatsData` as a subclass of `Dataset`. \n",
    "The dataset subclass holds the actual data, or at least provides access to it.\n",
    "To make it work with the `DataLoader` class we need to implement two methods:\n",
    "\n",
    "- `__getitem__(self, index)`: return the `index`'th sample, i.e. a single pair of (image, label)\n",
    "- `__len__(self)`: simply return the total number of samples $N$ in the dataset.\n",
    "\n",
    "These methods are so called Python \"magic\" methods, signified by the leading and closing double underscores.\n",
    "They typically enable special syntax for a class: `__getitem__` enables indexing of a class, and `__len__` enables calling the `len` function:\n",
    "```python\n",
    "# Consider an instance `data` of a class `MyDataset` implementing `__getitem__` and `__len__`\n",
    "data[10] # returns the item with index 10 in `data`\n",
    "len(data) # returns the length/size of `data`\n",
    "```\n",
    "We will return to why these are needed in the `DataLoader` wrapping class\n",
    "\n",
    "Now, to the actual implementation: The idea is to have the dataset class only store the filenames of the images (and the corresponding label), not the images themselves. We will find and store the filenames in the constructor. The `__getitem__` method will use the index to look up the correct filename and load it into memory.\n",
    "The `__len__` method is left for you to implement.\n",
    "\n",
    "Being able to use and understand code you have not written is an important ability. Below you are required to interact with the dataset class with a simple completion of the implementation and by extracting some data from the class. This is partly to ensure that you understand this specific class and partly to show you some tools for exploring new code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7bdbd0ff77605a6fb87494c081ecc2fc",
     "grade": false,
     "grade_id": "cell-5593ecac89fb79b1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class DogsCatsData(Dataset):\n",
    "    def __init__(self, root, transform, dog_label=1, cat_label=0):\n",
    "        \"\"\"Constructor\n",
    "\n",
    "        Args:\n",
    "            root (Path/str): Filepath to the data root, e.g. './small_train'\n",
    "            transform (Compose): A composition of image transforms, see below.\n",
    "        \"\"\"\n",
    "\n",
    "        root = Path(root)\n",
    "        if not (root.exists() and root.is_dir()):\n",
    "            raise ValueError(f\"Data root '{root}' is invalid\")\n",
    "\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self._dog_label = dog_label\n",
    "        self._cat_label = cat_label\n",
    "\n",
    "        # Collect samples, both cat and dog and store pairs of (filepath, label) in a simple list.\n",
    "        self._samples = self._collect_samples()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Get sample by index\n",
    "\n",
    "        Args:\n",
    "            index (int)\n",
    "\n",
    "        Returns:\n",
    "             The index'th sample (Tensor, int)\n",
    "        \"\"\"\n",
    "        # Access the stored path and label for the correct index\n",
    "        path, label = self._samples[index]\n",
    "        # Load the image into memory\n",
    "        img = Image.open(path)\n",
    "        # Perform transforms, if any.\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Total number of samples\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    def _collect_samples(self):\n",
    "        \"\"\"Collect all paths and labels\n",
    "\n",
    "        Helper method for the constructor\n",
    "        \"\"\"\n",
    "        # Iterator over dog filpath\n",
    "        dog_paths = self._collect_imgs_sub_dir(self.root / \"dogs\")\n",
    "        # Iterator of pairs (path, dog label)\n",
    "        # Again, we use the `map` function to create an iterator. It's use is not as common as the so called\n",
    "        # 'list comprehension' you've previously seen, but a good alternative to have seen.\n",
    "        dog_paths_and_labels = map(lambda path: (path, self._dog_label), dog_paths)\n",
    "        # Same for cats\n",
    "        cat_paths = self._collect_imgs_sub_dir(self.root / \"cats\")\n",
    "        cat_paths_and_labels = map(lambda path: (path, self._cat_label), cat_paths)\n",
    "        # Sorting is not strictly necessary, but filesystem globbing (wildcard search) is not deterministic,\n",
    "        # and consistency is nice when debugging.\n",
    "        return sorted(\n",
    "            list(chain(dog_paths_and_labels, cat_paths_and_labels)),\n",
    "            key=lambda x: x[0].stem,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _collect_imgs_sub_dir(sub_dir: Path):\n",
    "        \"\"\"Collect image paths in a directory\n",
    "\n",
    "        Helper method for the constructor\n",
    "        \"\"\"\n",
    "        if not sub_dir.exists():\n",
    "            raise ValueError(\n",
    "                f\"Directory '{sub_dir}' does not exist. Are you sure you have the correct path?\"\n",
    "            )\n",
    "        return sub_dir.glob(\"*.jpg\")\n",
    "\n",
    "    def get_sample_by_id(self, id_):\n",
    "        \"\"\"Get sample by image id\n",
    "\n",
    "        Convenience method for exploration.\n",
    "        The indices does not correspond to the image id's in the filenames.\n",
    "        Here is a (rather inefficient) way of inspecting a specific image.\n",
    "\n",
    "        Args:\n",
    "            id_ (str): Image id, e.g. `dog.321`\n",
    "        \"\"\"\n",
    "        id_index = [path.stem for (path, _) in self._samples].index(id_)\n",
    "        return self[id_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a3743002d007d4a09e82e17870d085ee",
     "grade": false,
     "grade_id": "cell-6a1f17f3e517a507",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(2 points)**\n",
    "\n",
    "Show that you understand the implementation by creating an instance called `example_dataset` of it. Create it from the small training set.\n",
    "Use the instance to:\n",
    "- print the number of samples in it\n",
    "- print the label of the second sample, note that this is a number (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "864ca8a3e8d8019f9d95d3ca9ee7deea",
     "grade": false,
     "grade_id": "cell-dfd1969f7ce6902a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# The Dataset constructor has a transform attribute, we will cover it below. Just use this for now:\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "transform = Compose([ToTensor()])\n",
    "example_dataset = None\n",
    "number_of_samples = None\n",
    "label = None\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(f\"The number of samples is: {number_of_samples}\")\n",
    "print(f\"The label of the second sample is: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec2904ad6e8ae11c477857465cab4188",
     "grade": true,
     "grade_id": "cell-5ea0670ba7b149e2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test case for `number_of_samples`\n",
    "ha1_tests.test_number_of_samples(example_dataset, number_of_samples, DogsCatsData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c679ef2328af69f1ce4c79b1bb49e11d",
     "grade": true,
     "grade_id": "cell-84ae0fe68c1797e6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test case for `label`\n",
    "ha1_tests.test_label(example_dataset, label, DogsCatsData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "32afdc1ceb5019325bbeee55c8f4cddf",
     "grade": false,
     "grade_id": "cell-e03f109baaddeb95",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "It is vital to explore your data, but it can be tricky to deal with images in the tensor format.\n",
    "To aid you, use the below helper function to visually inspect your images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(axis, image_tensor):\n",
    "    \"\"\"Display a tensor as an image\n",
    "\n",
    "    Args:\n",
    "        axis (pyplot axis)\n",
    "        image_tensor (torch.Tensor): tensor with shape (num_channels=3, width, heigth)\n",
    "    \"\"\"\n",
    "\n",
    "    # See hint above\n",
    "    if not isinstance(image_tensor, torch.Tensor):\n",
    "        raise TypeError(\n",
    "            \"The `display_image` function expects a `torch.Tensor` \"\n",
    "            + \"use the `ToTensor` transformation to convert the images to tensors.\"\n",
    "        )\n",
    "\n",
    "    # The imshow commands expects a `numpy array` with shape (3, width, height)\n",
    "    # We rearrange the dimensions with `permute` and then convert it to `numpy`\n",
    "    image_data = image_tensor.permute(1, 2, 0).numpy()\n",
    "    height, width, _ = image_data.shape\n",
    "    axis.imshow(image_data)\n",
    "    axis.set_xlim(0, width)\n",
    "    # By convention when working with images, the origin is at the top left corner.\n",
    "    # Therefore, we switch the order of the y limits.\n",
    "    axis.set_ylim(height, 0)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "_, axis = plt.subplots()\n",
    "some_random_index = 453\n",
    "# Here we use the __getitem__ method as a \"magic\" method.\n",
    "# Implementing it for a class, enables square bracket '[]' indexing\n",
    "image_tensor, label = example_dataset[some_random_index]\n",
    "display_image(axis, image_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Preprocessing \n",
    "The `DogsCatsData` class constructor has an argument called `transform`. It allows us to transform or preprocess all the images in a batch, from the raw image data to a more suitable format. There are multiple motivations for preprocessing:\n",
    "\n",
    "- Some transformations might be needed to actually make the data work with our network (reshaping, permuting dimensions et c.).\n",
    "- Make the training more efficient by making the input dimensions smaller, e.g. resizing, cropping.\n",
    "- Artificially expanding the training data through [data augmentation](https://cartesianfaith.com/2016/10/06/what-you-need-to-know-about-data-augmentation-for-machine-learning/)\n",
    "- We have some clever idea of how to change the data to create a simpler optimisation problem.\n",
    "\n",
    "We do not expect you to do data augmentation, but feel free to preprocess the data as you see fit. Use the [documentation](https://pytorch.org/vision/stable/transforms.html#torchvision-transforms) to view available transforms. Extra important is the `Compose` transformation, which is a meta-transformation which composes actual ones, and the `ToTensor` transformation which is the simplest way to go from image to tensor format.\n",
    "\n",
    "\n",
    "Hints:\n",
    "- Revisit the `DogsCatsData` example usage to see how to use the `Compose` and `ToTensor` transformations.\n",
    "- When feeding the images to your CNN, you'll probably want all of them to have the same spatial size, even though the .jpeg files differ in this. Resizing the images can be done using the previously mentioned Pytorch Transforms.\n",
    "- Resizing the images to a smaller size while loading them can be beneficial as it speeds up training. The CNN's do surprisingly well on 64x64 or even 32x32 images. Shorter training cycles give you more time to experiment! Note: The VGG network used later in this assignment is specialised for images that are 224x224.\n",
    "\n",
    "We encourage you to explore the data and choose transformations that you believe to be useful. For exploration we provide you with a helper function to visually compare transformations side by side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a05c7aa4bbeb47d9a082472a67be3cd",
     "grade": false,
     "grade_id": "cell-5ca8fc808d4ee65b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compare_transforms(transformations, index):\n",
    "    \"\"\"Visually compare transformations side by side.\n",
    "    Takes a list of DogsCatsData datasets with different compositions of transformations.\n",
    "    It then display the `index`th image of the dataset for each transformed dataset in the list.\n",
    "\n",
    "    Example usage:\n",
    "        compare_transforms([dataset_with_transform_1, dataset_with_transform_2], 0)\n",
    "\n",
    "    Args:\n",
    "        transformations (list(DogsCatsData)): list of dataset instances with different transformations\n",
    "        index (int): Index of the sample in the dataset you wish to compare.\n",
    "    \"\"\"\n",
    "\n",
    "    # Here we combine two functions from basic python to validate the input to the function:\n",
    "    # - `all` takes an iterable (something we can loop over, e.g. a list) of booleans\n",
    "    #    and returns True if every element is True, otherwise it returns False.\n",
    "    # - `isinstance` checks whether a variable is an instance of a particular type (class)\n",
    "    if not all(isinstance(transf, Dataset) for transf in transformations):\n",
    "        raise TypeError(\n",
    "            \"All elements in the `transformations` list need to be of type Dataset\"\n",
    "        )\n",
    "\n",
    "    num_transformations = len(transformations)\n",
    "    fig, axes = plt.subplots(1, num_transformations)\n",
    "\n",
    "    # This is just a hack to make sure that `axes` is a list of the same length as `transformations`.\n",
    "    # If we only have one element in the list, `plt.subplots` will not create a list of a single axis\n",
    "    # but rather just an axis without a list.\n",
    "    if num_transformations == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for counter, (axis, transf) in enumerate(zip(axes, transformations)):\n",
    "        axis.set_title(f\"transf: {counter}\")\n",
    "        image_tensor = transf[index][0]\n",
    "        display_image(axis, image_tensor)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4471e55aecbbd831017892bec0c17817",
     "grade": false,
     "grade_id": "cell-31b81f052b6e681e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Explore your dataset in this cell, you do not need to produce any results.\n",
    "img_size = None\n",
    "train_dataset = None\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dd21441bc6c76ab1f8f7329f3d853356",
     "grade": false,
     "grade_id": "cell-24463974ae20a276",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(2 points)**\n",
    "\n",
    "Normalisation of the training data is popular in pre-processing. What is the argument or intuition for why this is a beneficial transformation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d31b77204866f3fa7ca5ca91d35f11ed",
     "grade": true,
     "grade_id": "cell-6509545d160bac4b",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3bd520cb37dd7d1943d9930e33c6f1c8",
     "grade": false,
     "grade_id": "cell-5553a56f43c9298a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.3 Data loaders\n",
    "With our dataset class implementation in place, creating a `DataLoader` instance is simple.\n",
    "\n",
    "The data loader class wraps the dataset and provides a way to iterate over batches in the training loop.\n",
    "To produce batches, it gets the total number of samples $N$ with the dataset's `__len__` method.\n",
    "It divides the indices $1, \\dots, N$ into equally sized index batches with $B$ (batch size) elements. A particular batch with pairs of image and label is created by calling the dataset's `__getitem__` method with the indices in the batch. NB: the last batch in an epoch might be smaller if $N$ is not divisible by $B$.\n",
    "\n",
    "Create the data loaders needed for training (use the small version of the data), in the cell below.\n",
    "The `DataLoader` class is documented [here](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) , but it's not that much to it.\n",
    "You simply create a data loader with a dataset instance and some other (self-explanatory) settings:\n",
    "\n",
    "```python\n",
    "train_dataloader = DataLoader(example_dataset, batch_size=batch_size, shuffle=True)\n",
    "```\n",
    "\n",
    "**(1 point)**\n",
    "\n",
    "Create data loaders required for training and validation.\n",
    "\n",
    "Hints:\n",
    "- The specified `batch_size` should be chosen so that you train fast but don't run out of memory. You need to figure this out empirically; start small and increase the batch size until you run out of memory. Beyond this pragmatic approach, feel free to contribute to the highly contested scientific debate about the relation between batch size and generalisation.\n",
    "- The `DataLoader` constructor takes an optional argument `num_workers`, which defaults to `0` if not provided. Setting a higher number creates multiple threads which load batches concurrently. This can speed up training considerably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "22e79e2b42cf4d63f058df1fdc5ec71d",
     "grade": false,
     "grade_id": "cell-051ee24a83af3cf8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "batch_size = None\n",
    "train_dataloader = None\n",
    "val_dataloader = None\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8acf9caa8d048cc8f7bbbb59439822e0",
     "grade": true,
     "grade_id": "cell-fc7b5e98d56f39ca",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test case for `train_dataloader`\n",
    "ha1_tests.test_dataloader(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4562bcd90996b04d2130b156d982465d",
     "grade": true,
     "grade_id": "cell-b056ed8ec22e7c22",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test case for `val_dataloader`\n",
    "ha1_tests.test_dataloader(val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fa03ac25ff22c4682caa637291818546",
     "grade": false,
     "grade_id": "cell-535b126ae5a48cf1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(0 points)**\n",
    "\n",
    "What batch size did you use and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "624d3a8eaa8e2815fea682004d7617b7",
     "grade": true,
     "grade_id": "cell-291a9b1c853b7cb4",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "17dbec4b255552e42fcb3ef9557aa311",
     "grade": false,
     "grade_id": "cell-c0bfc1ac7fadfcc7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 2. Training\n",
    "\n",
    "### 2.1 The first model\n",
    "\n",
    "**(3 points)**\n",
    "\n",
    "Now, it's time to create a model called `FirstCnn`. To begin with, you have to create a CNN to an exact specification. After that, you will get the chance to be more creative.\n",
    "\n",
    "For the first model, create a network that:\n",
    "- Inherits from [`nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module)\n",
    "- Implements a constructor `__init__(self, img_size)`, a `forward(self, input_batch)` method and whatever other helper methods you deem necessary. Note that a (square) image size should be a parameter in the model's constructor. While not strictly necessary, it is an acceptable way of handling varying input dim's and it is convenient for testing.\n",
    "- Can handle square images of arbitrary size and arbitrary large mini-batch sizes (within memory limits, of course). You may assume that there are always three colour channels, i.e., a mini-batch will have the shape `(batch size = B, num channels = 3, img size = D, D)`\n",
    "- Has layers:\n",
    "    1. Two convolutional layers, each with 10 filters, kernel size = 3, stride = 1, padding = 0\n",
    "    2. A single fully connected layer.\n",
    "    - *Note:*\n",
    "        - Related layers such as a pooling operation are optional.\n",
    "        - Choose suitable activation functions.\n",
    "        - Take the layers from [`torch.nn`](https://pytorch.org/docs/stable/nn.html) module, and **not** from `torch.nn.functional` module. \n",
    "- Outputs the probability of the image belonging to the class 'dog'. Technically the output should consist of `B` probabilities, one for each image in the mini-batch and so have the shape `(B,)`.\n",
    "\n",
    "Hints:\n",
    "\n",
    "- The subpage for [`torch.nn`](https://pytorch.org/docs/stable/nn.html) is a good place to find the layer specifics.\n",
    "- Going from the last CNN layer to the final fully connected layer is not trivial. The convolutions produces feature maps which we can think of as an image with many channels, while the fully connected layer expects a row vector as input. Calculate how many output neurons the convolutions produce and use `.reshape` to make your tensor fit the fully connected layer. It is also common to see the `.view` and `.squeeze` methods to do the same thing. They basically do the same thing (apart from some differences in internal memory management) but are less transparent. *Hint within the hint:* remember that the fully connected layers expects a *batch* of 1D tensors. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "abf5e7561e4d94f93a44ef149c6bb5e7",
     "grade": false,
     "grade_id": "cell-4c9de348cd8bc4ff",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# class FirstCnn...\n",
    "#    def __init__(self, img_size):\n",
    "#       ...\n",
    "#    ...\n",
    "\n",
    "# first_model = FirstCnn(img_size)\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "90e09b40da6bc9432adc344938e17305",
     "grade": true,
     "grade_id": "cell-705319ff50688ca7",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test case 1 for `FirstCnn`\n",
    "# Note that the test takes the actual class, not an instance of it, as input.\n",
    "ha1_tests.test_model(FirstCnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "71fe5d14be338b75d417db38afb674d4",
     "grade": true,
     "grade_id": "cell-2a2848983c36bce0",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test case 2 for `FirstCnn`\n",
    "ha1_tests.test_architecture(FirstCnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "69cf3db1d990856678c29f353eaa8fc3",
     "grade": false,
     "grade_id": "cell-12ad4c02e8150588",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(2 points)**\n",
    "\n",
    "You have been told that one of the benefits of CNN is that it can handle input of different sizes. Yet, you needed to know the image size in the constructor.\n",
    "Explain how you made your model handle different input sizes and why it is necessary, despite it being a CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ef6f47ed6674c7abb17dc2e8a2303e6a",
     "grade": true,
     "grade_id": "cell-c06d2ae30ee4c649",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8b48b239e75bd2e1e2e6740d2be6b8d2",
     "grade": false,
     "grade_id": "cell-cb6fc78116ad6b75",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 2.2 The training loop\n",
    "\n",
    "**(2 points)**\n",
    "\n",
    "You have already seen quite a few training loops in the preparations. Below we provide you with an example of a basic one that you can use.\n",
    "You need to provide an implementation that maps network outputs (probabilites) to hard labels.\n",
    "\n",
    "An extra point is awarded if you provide a vectorised implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "938dd77454b83dc2a44c2b8432825493",
     "grade": true,
     "grade_id": "cell-4b4d93eec45833cf",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def output_to_label(z):\n",
    "    \"\"\"Map network output z to a hard label {0, 1}\n",
    "\n",
    "    Args:\n",
    "        z (Tensor): Probabilities for each sample in a batch.\n",
    "    Returns:\n",
    "        c (Tensor): Hard label {0, 1} for each sample in a batch\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d6c8fbbd8b4c69d7190b0ee9185d229d",
     "grade": true,
     "grade_id": "cell-27b0f6895c1bf5bb",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test case for `output_to_label` function\n",
    "ha1_tests.test_output_to_label(output_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "76004f3c9d404a6ab6d764353a927a6e",
     "grade": false,
     "grade_id": "cell-8a973f2f470ca603",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def training_loop(\n",
    "    model, optimizer, loss_fn, train_loader, val_loader, num_epochs, print_every\n",
    "):\n",
    "    print(\"Starting training\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    train_losses, train_accs, val_losses, val_accs = [], [], [], []\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model, train_loss, train_acc = train_epoch(\n",
    "            model, optimizer, loss_fn, train_loader, val_loader, device, print_every\n",
    "        )\n",
    "        val_loss, val_acc = validate(model, loss_fn, val_loader, device)\n",
    "        print(\n",
    "            f\"Epoch {epoch}/{num_epochs}: \"\n",
    "            f\"Train loss: {sum(train_loss)/len(train_loss):.3f}, \"\n",
    "            f\"Train acc.: {sum(train_acc)/len(train_acc):.3f}, \"\n",
    "            f\"Val. loss: {val_loss:.3f}, \"\n",
    "            f\"Val. acc.: {val_acc:.3f}\"\n",
    "        )\n",
    "        train_losses.extend(train_loss)\n",
    "        train_accs.extend(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "    return model, train_losses, train_accs, val_losses, val_accs\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "    model, optimizer, loss_fn, train_loader, val_loader, device, print_every\n",
    "):\n",
    "    # Train:\n",
    "    model.train()\n",
    "    train_loss_batches, train_acc_batches = [], []\n",
    "    num_batches = len(train_loader)\n",
    "    for batch_index, (x, y) in enumerate(train_loader, 1):\n",
    "        inputs, labels = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        z = model.forward(inputs)\n",
    "        loss = loss_fn(z, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_batches.append(loss.item())\n",
    "\n",
    "        hard_preds = output_to_label(z)\n",
    "        acc_batch_avg = (hard_preds == labels).float().mean().item()\n",
    "        train_acc_batches.append(acc_batch_avg)\n",
    "\n",
    "        # If you want to print your progress more often than every epoch you can\n",
    "        # set `print_every` to the number of batches you want between every status update.\n",
    "        # Note that the print out will trigger a full validation on the full val. set => slows down training\n",
    "        if print_every is not None and batch_index % print_every == 0:\n",
    "            val_loss, val_acc = validate(model, loss_fn, val_loader, device)\n",
    "            model.train()\n",
    "            print(\n",
    "                f\"\\tBatch {batch_index}/{num_batches}: \"\n",
    "                f\"\\tTrain loss: {sum(train_loss_batches[-print_every:])/print_every:.3f}, \"\n",
    "                f\"\\tTrain acc.: {sum(train_acc_batches[-print_every:])/print_every:.3f}, \"\n",
    "                f\"\\tVal. loss: {val_loss:.3f}, \"\n",
    "                f\"\\tVal. acc.: {val_acc:.3f}\"\n",
    "            )\n",
    "\n",
    "    return model, train_loss_batches, train_acc_batches\n",
    "\n",
    "\n",
    "def validate(model, loss_fn, val_loader, device):\n",
    "    val_loss_cum = 0\n",
    "    val_acc_cum = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_index, (x, y) in enumerate(val_loader, 1):\n",
    "            inputs, labels = x.to(device), y.to(device)\n",
    "            z = model.forward(inputs)\n",
    "\n",
    "            batch_loss = loss_fn(z, labels.float())\n",
    "            val_loss_cum += batch_loss.item()\n",
    "            hard_preds = output_to_label(z)\n",
    "            acc_batch_avg = (hard_preds == labels).float().mean().item()\n",
    "            val_acc_cum += acc_batch_avg\n",
    "    return val_loss_cum / len(val_loader), val_acc_cum / len(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ce592e703ca26ec946e0e409bbaf75a1",
     "grade": false,
     "grade_id": "cell-3a2bedde765aa366",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(1 point)**\n",
    "\n",
    "Use the training loop to train your model, using the two dataloaders you created earlier. Train for a reasonable amount of epochs, so as to get a good sense of how well this architecture performs.\n",
    "\n",
    "Hints:\n",
    "- Which `loss_fn` should you use? Think first about what is suitable for this problem, not what seems to work better empirically.\n",
    "- Training on a CPU is slow and in the beginning you just want to verify that your architecture actually produces a predicition with the correct shape. Make everything you can to speed up the prototyping phase, e.g. train only for a single epoch and make the images ridiculously small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c2f000acd2231fc932824882953662b9",
     "grade": true,
     "grade_id": "cell-0b7b0d0139ba368a",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = None\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "59c2bfceb5e354843a529efb9adead8c",
     "grade": false,
     "grade_id": "cell-9e2ca0ae153d493f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(0 points)**\n",
    "\n",
    "Which loss function did you use and why? In our \"Cats vs Dogs\" problem, the dataset is balanced in terms of classes. What would you change if there was a class imbalance (for example, if we had much less dog images)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "771d557508267e17eb458b486370a4a2",
     "grade": true,
     "grade_id": "cell-61e251f94f60d693",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ae223a4e45cb50a5e2c7ae95316b9cee",
     "grade": false,
     "grade_id": "cell-4d42c86687697a67",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 2.3 Visualisation\n",
    "\n",
    "**(1 point)**\n",
    "\n",
    "Create two plots. In one of them, plot the loss in the training and the validation datasets. In the other one, plot the accuracy in the training and validation datasets.\n",
    "Note that the given training loop produces metrics at different intervals for training and validation, make sure that you align your metrics in a way that makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f6d6f92390994507b50e9128673a9ad",
     "grade": true,
     "grade_id": "cell-fa81712e1e27432a",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "800e07a51634b18a8b6f3d6ebbaaa2cd",
     "grade": false,
     "grade_id": "cell-f2fc166890962bcf",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(2 points)**\n",
    "\n",
    "Based on these, what would you suggest for improving your model? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2b42c08ae7bcb61726d89b8dd46922d6",
     "grade": true,
     "grade_id": "cell-506e21ce469b67f5",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2feb5390fe765e9bdd51ad1c0598c4e3",
     "grade": false,
     "grade_id": "cell-7e15f27d83e958ff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "[Save your model](https://pytorch.org/tutorials/beginner/saving_loading_models.html) to disk (the architecture, weights, optimizer state, losses and accuracies). This is simply so you can use it again easily in the later parts of the notebook, without having to keep it in memory or re-training it. The actual file you create is not relevant to your submission. The code to save the model is given in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming that you named your model \"first_model\" and training statistics as following\n",
    "torch.save(\n",
    "    {\n",
    "        \"model_state_dict\": first_model.state_dict(),\n",
    "        \"train_losses\": first_train_losses,\n",
    "        \"train_accs\": first_train_accs,\n",
    "        \"val_losses\": first_val_losses,\n",
    "        \"val_accs\": first_val_accs,\n",
    "    },\n",
    "    \"./first_model.ckpt\",\n",
    ")\n",
    "\n",
    "# Example of creating and initialising model with a previously saved state dict:\n",
    "saved_first_model = FirstCnn(img_size)  # fill-in the arguments if needed\n",
    "checkpoint = torch.load(\"first_model.ckpt\")\n",
    "saved_first_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "# Example of re-using saved statistics:\n",
    "print(\"Validation accuracies:\", checkpoint[\"val_accs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "afa8e2c066a79d25a5ad58e8095cbfac",
     "grade": false,
     "grade_id": "cell-ee79a83a62b70a8f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 3. Improving your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2eebd5034e923991c28bcf7703749d95",
     "grade": false,
     "grade_id": "cell-5314d286e79e0377",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(3 points)**\n",
    "\n",
    "Now you are free to create whichever model you want. A simple improvement based on your analysis of the above results is fine, or you can do something more ambitious. When you're happy with one architecture, copy it in the cell below and train it here. Save the training and validation losses and accuracies. You'll use this later to compare your best model with the one using transfer learning. *Hint*: reducing the input image size facilitates training.\n",
    "\n",
    "**NOTE**: When trying different ideas, you'll end up with several different models. However, when submitting your solutions to Canvas, the cell below must contain only the definition and training of **one model**. Remove all code related to the models that were not chosen.\n",
    "\n",
    "**NOTE 2**: It may feel like you are stuck trying to make the model perform better. Understanding the mechanics and a lot of experimentation will help gain intuition. But it is also beneficial for you to explore some of the popular architectures, building blocks, and techniques (e.g., residual blocks, Inception modules, dilated convolutions, depthwise separable convolutions, data augmentation etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85c1e00c14569b06324d5bbab5c3edea",
     "grade": true,
     "grade_id": "cell-6edb7d7e343ab14b",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# class ImprovedCnn...\n",
    "#    def __init__...\n",
    "#       ...\n",
    "#    ...\n",
    "\n",
    "# impr_model = ImprovedCnn(...\n",
    "    \n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "69b939be5baf799c86295d955b469f19",
     "grade": false,
     "grade_id": "cell-d033937b5a8b9875",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(1 point)**\n",
    "\n",
    "Create two plots. In one of them, plot the loss in the training and the validation datasets. In the other one, plot the accuracy in the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "87dc3f33103ce629dd23fd18cc68c5d7",
     "grade": true,
     "grade_id": "cell-3df999674672de47",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6c29f862a24d99266165631ed7d3c863",
     "grade": false,
     "grade_id": "cell-7e5eb612469340fb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(1 point)**\n",
    "\n",
    "How does the model perform, compared to the first CNN model? Create one plot with the training accuracy and another with the validation accuracy of the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb47a5e3ba54123363f0b15d9df6792c",
     "grade": true,
     "grade_id": "cell-4da6b3b63e3305f0",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ee4b2624bc239fe6118b641f5b272f7f",
     "grade": false,
     "grade_id": "cell-a827c39d9e652e52",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(1 point)**\n",
    "\n",
    "Did your results improve? What problems did your improvements fix? Explain why, or why not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "48d17550761a4b011b9b8411f9219133",
     "grade": true,
     "grade_id": "cell-cbda4b585ad39ddc",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e6648c8efa74adb828b4413fe6f250f1",
     "grade": false,
     "grade_id": "cell-c67bcc4fbec1808e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Save your model to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "696f87bee7f1d23b60d147ef8bc30686",
     "grade": false,
     "grade_id": "cell-8f5f9327386823da",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Assuming that you named your model and statistics as \"impr_...\"\n",
    "torch.save(\n",
    "    {\n",
    "        \"model_state_dict\": impr_model.state_dict(),\n",
    "        \"train_losses\": impr_train_losses,\n",
    "        \"train_accs\": impr_train_accs,\n",
    "        \"val_losses\": impr_val_losses,\n",
    "        \"val_accs\": impr_val_accs,\n",
    "    },\n",
    "    \"./improved_model.ckpt\",\n",
    ")\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ceb570afa39d746b3b5f132ecb5bc72e",
     "grade": false,
     "grade_id": "cell-25f9cc8d17491d0d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 4. Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1f8a14db4f089fe68b5a80cf0fd35a80",
     "grade": false,
     "grade_id": "cell-cf9b347fc3ee9255",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**From now, training on a CPU will likely not be feasible. If your computer has a GPU, try it out! Otherwise, now is the time to connect to a server/cloud instance**\n",
    "\n",
    "Now, instead of trying to come up with a good architecture for this task, we'll use the VGG16 architecture, but with the top layers removed (the fully connected layers + softmax). We'll substitute them with our own top network, designed for dog/cat classification.\n",
    "This top network is often called the \"head\" in transfer learning.\n",
    "\n",
    "However, this model has a very high capacity, and will probably suffer a lot from overfitting if we try to train it from scratch, using only our small subset of data. Instead, we'll start the optimization with the weights obtained after training VGG16 on the ImageNet dataset.\n",
    "\n",
    "Start by loading the *pretrained* VGG16 model, from the [torchvision.models](https://pytorch.org/vision/stable/models.html?highlight=vgg#torchvision.models.vgg16)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "vgg_model = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# If you had to downgrade Python+torch+torchvision\n",
    "# (i.e. your operating system does not support the provided environment file):\n",
    "# the syntax for loading vgg16 weights is different in older version:\n",
    "#     vgg_model = models.vgg16(pretrained=True)\n",
    "\n",
    "print(vgg_model.classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "363693f578752738d9abb7d25bdccfea",
     "grade": false,
     "grade_id": "cell-faed8047ef25a60d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(1 point)**\n",
    "\n",
    "Create a new model named `head` as a so-called head network to the base VGG model.\n",
    "VGG is a relatively deep network so think about what would be a reasonable model head for transfer learning.\n",
    "\n",
    "*Hint:*\n",
    "- You can access and modify the top layers of the VGG model with `vgg_model.classifier`, and the remaining layers with `vgg_model.features`.\n",
    "- You can get the number of output features of `vgg_model.features` with `vgg_model.classifier[0].in_features`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5081a7ef50033658c18ef9dda5c90011",
     "grade": false,
     "grade_id": "cell-56cb37360051a638",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "head = None\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "88cf31d808e77387b240ae41162e85d7",
     "grade": true,
     "grade_id": "cell-f243d78a4ccfbdcd",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test case for `head`\n",
    "ha1_tests.test_transfer_learning_head(head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5b68ddca6ab289a1544543a00347f543",
     "grade": false,
     "grade_id": "cell-d746f9eb61e3ea44",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(2 points)**\n",
    "\n",
    "Now add the new model on top of VGG. You full model should be stored in the `vgg_model` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "43288d166ad13535197e39fccad8c7ac",
     "grade": false,
     "grade_id": "cell-76e4aad7fbcf5d05",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8e6e0fc567cdd70431145f264e6268f5",
     "grade": true,
     "grade_id": "cell-5dc53a5e852660ab",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test case 1 for `vgg_model`\n",
    "ha1_tests.test_vgg_model_1(vgg_model, head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "371ebe6362e89970e86d7ca34f47a2bb",
     "grade": true,
     "grade_id": "cell-0935d3579b6b2159",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test case 2 for `vgg_model`\n",
    "ha1_tests.test_vgg_model_2(vgg_model, head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0ff2dfca08a9262327d0ee7d0c58b2bd",
     "grade": false,
     "grade_id": "cell-f76d1a7f6280af0d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 4.1 Using VGG features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed609b8d5b5f75b2980793cc2f507b34",
     "grade": false,
     "grade_id": "cell-270f8ec140ddfba3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(1 point)**\n",
    "\n",
    "Now we're almost ready to train the new model. For transfer learning we want to freeze all but the top layers in your architecture (i.e. signal to the optimizer that the bottom layers should not be changed during optimization). We do that by setting the attribute `requires_grad` of every parameter in the feature sub-network of `vgg_model` to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2735c800ef1e589b60d2d02462a24a87",
     "grade": false,
     "grade_id": "cell-bfb58ea46c31df0a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "99b0ee394fdfd841e6b810699c48c81b",
     "grade": true,
     "grade_id": "cell-427cfc17a5907abd",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test case for `vgg_model`\n",
    "ha1_tests.test_vgg_model_parameters_for_transfer_learning(vgg_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a2baa4df150ba7a051a5793a8bb0a769",
     "grade": false,
     "grade_id": "cell-ad32824e875c79cd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(1 point)**\n",
    "\n",
    "Prepare dataloaders for transfer learning. Don't forget that the VGG network is specialised for images of a certain size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ccddb302494ec15268661e04a635c188",
     "grade": false,
     "grade_id": "cell-6e509c469c8a52af",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train_dataloader = None\n",
    "val_dataloader = None\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "155670a563d5d2d3712ff5327e91138c",
     "grade": true,
     "grade_id": "cell-89b4dacf58306e03",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test case for `train_dataloader`\n",
    "ha1_tests.test_dataloader_for_transfer_learning(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "707b95c884ffe58a88c3061d779f9ddd",
     "grade": true,
     "grade_id": "cell-c42fbfb7ac764522",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test case for `val_dataloader`\n",
    "ha1_tests.test_dataloader_for_transfer_learning(val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d9545392e82eacd229bc94eb7a7d47b7",
     "grade": false,
     "grade_id": "cell-b508ede3d760a86b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(1 point)**\n",
    "\n",
    "Perform the transfer learning by training the top layers of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "92df0b1dc243a6d6c9d4fbbdfc325ed9",
     "grade": true,
     "grade_id": "cell-f50c3d451530b9a8",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "optimizer = None\n",
    "num_epochs = None\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3e923dc4cde7941eeba4c832f6d3c284",
     "grade": false,
     "grade_id": "cell-ad79e1aa5c4a6185",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(1 point)**\n",
    "\n",
    "Create two plots. In one of them, plot the loss in the training and the validation datasets. In the other one, plot the accuracy in the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8c941425a83b8694e580f44586efac41",
     "grade": true,
     "grade_id": "cell-f17c882b2a09dee7",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b3b189c1f2b16a903cef2823584c162f",
     "grade": false,
     "grade_id": "cell-779d477ffe1ebbf6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(1 point)**\n",
    "\n",
    "How does the model perform, compared to the model obtained in step 3? Create one plot with the training accuracy and another with the validation accuracy of the two scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "463e33fdb695c17d7a4bcafe7d861035",
     "grade": true,
     "grade_id": "cell-e3e3990ba39bea67",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ae6448a29be16e018137cb0e99cff84f",
     "grade": false,
     "grade_id": "cell-b84dd461d5ddcc8d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(2 point)**\n",
    "\n",
    "Compare these results.\n",
    "\n",
    "- Which approach worked best, starting from scratch or doing transfer learning?\n",
    "- Reflect on whether your comparison is fair or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dce3df56e420472eafb032186728064f",
     "grade": true,
     "grade_id": "cell-f9e1a6a643946cd2",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c872a8f240be2253003731479b4bf613",
     "grade": false,
     "grade_id": "cell-c8afb448c67da5f8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(2 points)**\n",
    "\n",
    "What are the main differences between the ImageNet dataset and the Dogs vs Cats dataset we used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d82a61ecfea4ff8b4f061a7ca888a455",
     "grade": true,
     "grade_id": "cell-2be321b63232ae01",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4144db7767eedb07c0ea1913a8064059",
     "grade": false,
     "grade_id": "cell-71a8b8de004f6e57",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(2 points)**\n",
    "\n",
    "Even though there are considerable differences between these datasets, why is it that transfer learning is still a good idea?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9605a949f6c6b94fac8feb5a6bcbbcad",
     "grade": true,
     "grade_id": "cell-655d00face15a862",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6e0873c0d5ca87ba30f0929328d5d0a0",
     "grade": false,
     "grade_id": "cell-19785940b9624d2c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(1 points)**\n",
    "\n",
    "In which scenario would transfer learning be unsuitable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ce818a8e86ceb511348597d975b34016",
     "grade": true,
     "grade_id": "cell-e79df7472ff5506a",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b24258079f4e71e1842b78e479095117",
     "grade": false,
     "grade_id": "cell-111f2b1d28919293",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Save the model to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "21141b44ffdb8df562dad77aa330661e",
     "grade": true,
     "grade_id": "cell-674350e34be30d10",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ab5bf17534c2ac6852d79e32793fdbf7",
     "grade": false,
     "grade_id": "cell-544a73726bebe121",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 4.2 Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "22cdae16575f45b916982a65f6bb7b63",
     "grade": false,
     "grade_id": "cell-1ee9ebc87fd3358e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now that we have a better starting point for the top layers, we can train the entire network. Unfreeze the bottom layers of `vgg_model` by resetting the `requires_grad` attribute to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4daef333e75c80e87a9ea3c950a232ee",
     "grade": false,
     "grade_id": "cell-3918c2cdd9817f7e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d22e8a6e036f94f620e2d862806eda9",
     "grade": true,
     "grade_id": "cell-66d1be1082fdb3a8",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test case for `vgg_model`\n",
    "ha1_tests.test_vgg_model_parameters_for_fine_tuning(vgg_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "89fe06ac7c2c3f9843190d8360b10dab",
     "grade": false,
     "grade_id": "cell-80fa8c89f1b262f1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(1 point)**\n",
    "\n",
    "Fine tune the model by training all the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2bef086ef5ecb931d32a9ec0f192bab5",
     "grade": true,
     "grade_id": "cell-594c6039216461e5",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = None\n",
    "optimizer = None\n",
    "num_epochs = None\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a38b1f2c96e6cd7a53da8f71057cb91e",
     "grade": true,
     "grade_id": "cell-91df7563b6077e8f",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test case for `learning_rate`\n",
    "ha1_tests.test_learning_rate(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "815dae0c86122c7f787a61126c239451",
     "grade": false,
     "grade_id": "cell-5dc3e388a41da3ed",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(1 point)**\n",
    "\n",
    "How does the model perform, compared to the model trained with frozen layers? Create one plot with the training accuracy and another with the validation accuracy of the two scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca07c6f01c099ca8174ca25fd8b2a179",
     "grade": true,
     "grade_id": "cell-7edb12ee397ec817",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "febc58a902a7439b6412be58cb6e2679",
     "grade": false,
     "grade_id": "cell-5dae528a81d5ff24",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(2 points)**\n",
    "\n",
    "Why is it a good idea to use a very small learning rate when doing fine tuning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c42e76bf52e85ffe40c42784a13fcec7",
     "grade": true,
     "grade_id": "cell-0f4a5edca490320e",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4971aa0a2e159c1780dedfc5e78b7c15",
     "grade": false,
     "grade_id": "cell-4ed3967e4f6c5f7f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Save the model to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fc1c3d05330d546d10fa60ac83f3418f",
     "grade": false,
     "grade_id": "cell-777d7ed9a3cbabd0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "42d6362e7a0f25fc579ad6e33f1b401b",
     "grade": false,
     "grade_id": "cell-56908ee1e60aa411",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 4.3 Improving the top model (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4f74ac0be60b7253bfe604c521647a07",
     "grade": false,
     "grade_id": "cell-3c8d8e5ab949ee35",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Improve the architecture for the layers you add on top of VGG16. Try different ideas! When you're happy with one architecture, copy it in the cell below and train it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4c69b20551001d0e612f3b9221dc7dbc",
     "grade": true,
     "grade_id": "cell-22d09c8401d84b61",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9dbd81b9e869b66b55d232c07d057651",
     "grade": false,
     "grade_id": "cell-48933baad6c5afeb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(1 point)**\n",
    "\n",
    "How does the model perform, compared to the model trained in step 4.2? Create one plot with the training accuracy and another with the validation accuracy of the two scenarios. A point is awarded if your improvement helped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b520759c1ceb8218d203dc9655d25361",
     "grade": true,
     "grade_id": "cell-7cb62a04916a848e",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "84662189498e2454093c4a54a53716d6",
     "grade": false,
     "grade_id": "cell-8bbfa3e11e2dfff9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Save the model to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2cec393e4f57e72ff91f5d55ad0dc14c",
     "grade": true,
     "grade_id": "cell-e64508c0fe4fa4f6",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "49c722f31cfd70e995b6226c86584565",
     "grade": false,
     "grade_id": "cell-ad0efbac33de5a65",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 5. Final training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6e460754d2c0f05f0e79ae982a3fe3d3",
     "grade": false,
     "grade_id": "cell-cf811afdac96843b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now we'll train the model that achieved the best performance so far using the entire dataset.\n",
    "\n",
    "**Note**: start the optimization with the weights you obtained training in the smaller subset, i.e. *not* from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4ad8b2d67a68a5afcf4c8645d3070550",
     "grade": false,
     "grade_id": "cell-3ae2a65188e4ac74",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "First, create two new data loaders, one for training samples and one for validation samples. This time, they'll load data from the folders for the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "afc667fc5653cb0121e628ca8c6e12f8",
     "grade": true,
     "grade_id": "cell-64eaa83780f5eac9",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "full_train_dataloader = None\n",
    "full_val_dataloader = None\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5d84d237947e7e263016bc22625ac97b",
     "grade": true,
     "grade_id": "cell-b2a4f81490a6a9ab",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test case for `full_train_dataloader` and `full_val_dataloader`\n",
    "ha1_tests.test_dataloaders_for_final_training(full_train_dataloader, full_val_dataloader, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0a0d2efc5e7ae9e9f5abbb3899ce1a0d",
     "grade": false,
     "grade_id": "cell-f3f79586de42561b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(1 point)**\n",
    "\n",
    "Train your model using the full data. This optimization might take a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aa1dbeccd938f930cf045698741c25be",
     "grade": true,
     "grade_id": "cell-c7dd71a632b5f152",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = None\n",
    "optimizer = None\n",
    "num_epochs = None\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b6aa939944e4e6d197d4717baf552545",
     "grade": false,
     "grade_id": "cell-b1861d3a543c6386",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(1 point)**\n",
    "\n",
    "How does the model perform now when trained on the entire dataset, compared to when only trained on the smaller subset of data? Create one plot with the training accuracy and another with the validation accuracy of the two scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4cb4b027dfb262697e04625c69fbf305",
     "grade": true,
     "grade_id": "cell-ceaac6be60ce36a9",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1bb0aeed891c6247e08b5b58439cfc17",
     "grade": false,
     "grade_id": "cell-b38092b08c150e7d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(2 points)**\n",
    "\n",
    "- What can you conclude from these plots?\n",
    "- Did you expect what you observe in the plots, explain!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "22d5b2529d702c64919bef4e02ca308c",
     "grade": true,
     "grade_id": "cell-694a3fbb7f081da8",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4d355c997a573273f5f0a6593a810816",
     "grade": false,
     "grade_id": "cell-5afc8b836cbbb30e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Save the model to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17f670d9293fcf83d7293a160647e5ab",
     "grade": false,
     "grade_id": "cell-012bfbd6ff78577a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "616d9047ba0c30d8343e48ecc58bd4d0",
     "grade": false,
     "grade_id": "cell-5e1ddfbfceb4d194",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 6. Evaluation on test set (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e59f727c60ea2cebec070efe2c6613ce",
     "grade": false,
     "grade_id": "cell-a97630bf5d85363f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now we'll evaluate your final model, obtained in step 6, on the test set. As mentioned before, the samples in the test set are not labelled, so we can't compute any supervised performance metrics ourselves. \n",
    "\n",
    "As a bit of fun and to inspire some friendly competition you may instead submit it to Kaggle for evaluation.\n",
    "\n",
    "**Feel free to experiment with different backbones and/or heads to create the best model you can.**\n",
    "\n",
    "Compute the predictions for all samples in the test set according to your best model, and save it in a .csv file with the format expected by the competition.\n",
    "\n",
    "For the test data we need a slightly different dataset class, due to the lack of labels in the data.\n",
    "A more proper way to implement it would be to make a common class which handles both the train and test settings.\n",
    "Here, we'll just copy the train dataset class and make some modifications to ignore the labels.\n",
    "\n",
    "Hints:\n",
    "- There is a `sampleSubmission.csv` file included in the zip data. Take a look at it to better understand what is the expected format here.\n",
    "- If you don't know how to create and write to files with Python, it's a well-behaved Google search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2cef0b2ba7d64e75c2522cf3345ffd74",
     "grade": true,
     "grade_id": "cell-cc77ac7849f856e1",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from time import time\n",
    "\n",
    "\n",
    "class TestData(Dataset):\n",
    "    def __init__(self, root: Path, transform):\n",
    "        root = Path(root)\n",
    "        if not (root.exists() and root.is_dir()):\n",
    "            raise ValueError(f\"Data root '{root}' is invalid\")\n",
    "\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self._samples = self._collect_samples()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self._samples[index]\n",
    "        num_id = int(path.stem)\n",
    "        img = Image.open(path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, num_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._samples)\n",
    "\n",
    "    def _collect_samples(self):\n",
    "        test_paths = self._collect_imgs_sub_dir(self.root)\n",
    "        return sorted(list(test_paths), key=lambda path: int(path.stem))\n",
    "\n",
    "    @staticmethod\n",
    "    def _collect_imgs_sub_dir(sub_dir: Path):\n",
    "        if not sub_dir.exists():\n",
    "            raise ValueError(f\"Data root '{sub_dir}' does not exist.\")\n",
    "        return sub_dir.glob(\"*.jpg\")\n",
    "\n",
    "    def get_sample_by_id(self, id_):\n",
    "        id_index = self._samples.index(id_)\n",
    "        return self[id_index]\n",
    "\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "97877f48922ebbd0c50829231a227ce5",
     "grade": false,
     "grade_id": "cell-faf8664f26ff7f4e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now that you created your submission file, submit it to Kaggle for evaluation. The [old competition](https://www.kaggle.com/c/dogs-vs-cats) does not allow submissions any more, but you can submit your file to the [new one](https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition) via the \"Late submission\" button (they use the same data). The Kaggle CLI can be used as well. Kaggle evaluates your submission according to your log-loss score. Which score did you obtain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b8507722245d56a20dd6809091664f78",
     "grade": true,
     "grade_id": "cell-e951dcec64dec85d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e8a7f3a8236f43994efe29067d7237c2",
     "grade": false,
     "grade_id": "cell-dc362abcfef32eae",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "What was the username you used for this submission?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8cc61665c676edcd9192df3c15714aa3",
     "grade": true,
     "grade_id": "cell-d519532bb1f957c3",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
